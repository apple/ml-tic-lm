accum_freq: 1
global_batch_size: 256
beta1: 0.9
beta2: 0.95
data_key: json.gz
epochs: 1  # This should match your desired number of epochs
ffn_type: swiglu_torch
attn_name: torch_attn
fsdp: true
fsdp_limit_all_gathers: true
fsdp_pure_bf16: true
fsdp_use_orig_params: true
grad_clip_norm: 1
log_every_n_steps: 20
lr: 3e-3
lr_cooldown_end: 3e-5
model: open_lm_3b
model_norm: gain_only_lp_layer_norm
precision: amp_bfloat16
qk_norm: true
report_to: wandb
wandb_project_name: tic_llm
seed: 124
train_num_samples: 223674572800 # This should match your total tokens divided by epochs if needed
warmup: 50
wd: 0.033
workers: 1
z_loss_coefficient: 1e-4
remote_sync: "s3://path/to/your/checkpoints"  # Set this in your run command with --remote-sync
remote_sync_frequency: 300
resume: latest
remote_sync_protocol: s3
log_logit_mean: true
data_tolerate_num_ckpts: 5
multiple_data_passes: true
data_tolerate_error_p: 0.90
torchcompile: true
backend_timeout: 3600
delete_previous_checkpoint: true
log_local: true
